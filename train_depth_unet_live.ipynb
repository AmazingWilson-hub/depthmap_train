{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grayscale Depth Training (UNet) — Live Curves\n",
        "\n",
        "Train a single-channel UNet for depth regression with **live loss/metric plots**.\n",
        "\n",
        "**Features**\n",
        "- Config cell to tweak params\n",
        "- Mixed precision on CUDA\n",
        "- L1 + SiLog loss\n",
        "- Per-epoch RMSE / AbsRel on val\n",
        "- **Live charts** updated every epoch\n",
        "- Save `best.pt`, `last.pt`, `history.csv`, and `curves.png`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_images': 'data_v2/train/images',\n",
              " 'train_depths': 'data_v2/train/depths',\n",
              " 'val_images': 'data_v2/val/images',\n",
              " 'val_depths': 'data_v2/val/depths',\n",
              " 'img_size': 384,\n",
              " 'max_depth': 80.0,\n",
              " 'batch': 16,\n",
              " 'epochs': 50,\n",
              " 'lr': 0.001,\n",
              " 'weight_decay': 0.0001,\n",
              " 'w_l1': 1.0,\n",
              " 'w_silog': 0.1,\n",
              " 'device': 'cuda',\n",
              " 'out_dir': 'runs/dep_unet_v2'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Config (EDIT) =====\n",
        "config = {\n",
        "    \"train_images\": \"data_v2/train/images\",\n",
        "    \"train_depths\": \"data_v2/train/depths\",\n",
        "    \"val_images\":   \"data_v2/val/images\",\n",
        "    \"val_depths\":   \"data_v2/val/depths\",\n",
        "    \"img_size\": 384,\n",
        "    \"max_depth\": 80.0,  # set to your dataset's ~P99\n",
        "    \"batch\": 16,\n",
        "    \"epochs\": 50,\n",
        "    \"lr\": 1e-3,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"w_l1\": 1.0,\n",
        "    \"w_silog\": 0.1,\n",
        "    \"device\": \"cuda\",           # 'cuda' on 4080S, else 'cpu'\n",
        "    \"out_dir\": \"runs/dep_unet_v2\"\n",
        "}\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===== Imports & utils =====\n",
        "import os, math, time, random, csv\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def valid_mask_from(gt: torch.Tensor):\n",
        "    return torch.isfinite(gt) & (gt > 0)\n",
        "\n",
        "def rmse(pred, gt, mask):\n",
        "    if mask.sum() == 0: return torch.tensor(float('nan'), device=pred.device)\n",
        "    return torch.sqrt(F.mse_loss(pred[mask], gt[mask]))\n",
        "\n",
        "def abs_rel(pred, gt, mask):\n",
        "    if mask.sum() == 0: return torch.tensor(float('nan'), device=pred.device)\n",
        "    return ((pred[mask] - gt[mask]).abs() / gt[mask].clamp_min(1e-6)).mean()\n",
        "\n",
        "def to_device(batch, device):\n",
        "    x,y,m = batch\n",
        "    return x.to(device, non_blocking=True), y.to(device, non_blocking=True), m.to(device, non_blocking=True)\n",
        "\n",
        "device = torch.device('cuda' if (config['device']=='cuda' and torch.cuda.is_available()) else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Dataset =====\n",
        "class ImageDepthNPY(Dataset):\n",
        "    def __init__(self, img_dir, dep_dir, img_size=384, max_depth=80.0, aug=False, allow_exts=None):\n",
        "        self.img_dir = Path(img_dir); self.dep_dir = Path(dep_dir)\n",
        "        self.img_size = int(img_size); self.max_depth = float(max_depth)\n",
        "        self.aug = bool(aug)\n",
        "        if allow_exts is None:\n",
        "            allow_exts=(\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\",\".webp\")\n",
        "        imgs = []\n",
        "        for p in self.img_dir.iterdir():\n",
        "            if p.is_file() and p.suffix.lower() in allow_exts: imgs.append(p)\n",
        "        self.samples = []\n",
        "        for p in imgs:\n",
        "            npy = self.dep_dir / (p.stem + '.npy')\n",
        "            if npy.exists(): self.samples.append((p, npy))\n",
        "        if not self.samples:\n",
        "            raise FileNotFoundError(f'No paired samples under {img_dir} & {dep_dir}')\n",
        "\n",
        "        self.geom_train = T.Compose([\n",
        "            T.RandomResizedCrop(self.img_size, scale=(0.7,1.0))\n",
        "        ]) if self.aug else T.Resize((self.img_size, self.img_size))\n",
        "\n",
        "        self.to_tensor_norm = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_p, dep_p = self.samples[idx]\n",
        "        pil = Image.open(img_p).convert('L')\n",
        "        pil = self.geom_train(pil)\n",
        "        x = self.to_tensor_norm(pil)\n",
        "\n",
        "        d = np.load(dep_p).astype(np.float32)\n",
        "        d = torch.from_numpy(d).unsqueeze(0).unsqueeze(0)\n",
        "        d = F.interpolate(d, size=(self.img_size, self.img_size), mode='bilinear', align_corners=False).squeeze(0)\n",
        "        m = valid_mask_from(d)\n",
        "        y = torch.zeros_like(d)\n",
        "        y[m] = (d[m] / config['max_depth']).clamp(0,1)\n",
        "\n",
        "        if self.aug and random.random()<0.5:\n",
        "            x = torch.flip(x, dims=[2])\n",
        "            y = torch.flip(y, dims=[2])\n",
        "            m = torch.flip(m, dims=[2])\n",
        "        return x,y,m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Model =====\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1), nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1), nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UNetSmall(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=1):\n",
        "        super().__init__()\n",
        "        ch=[64,128,256,512]\n",
        "        self.d1 = DoubleConv(in_ch, ch[0]); self.p1 = nn.MaxPool2d(2)\n",
        "        self.d2 = DoubleConv(ch[0], ch[1]); self.p2 = nn.MaxPool2d(2)\n",
        "        self.d3 = DoubleConv(ch[1], ch[2]); self.p3 = nn.MaxPool2d(2)\n",
        "        self.d4 = DoubleConv(ch[2], ch[3])\n",
        "        self.u3 = nn.ConvTranspose2d(ch[3], ch[2], 2, stride=2); self.dc3 = DoubleConv(ch[2]*2, ch[2])\n",
        "        self.u2 = nn.ConvTranspose2d(ch[2], ch[1], 2, stride=2); self.dc2 = DoubleConv(ch[1]*2, ch[1])\n",
        "        self.u1 = nn.ConvTranspose2d(ch[1], ch[0], 2, stride=2); self.dc1 = DoubleConv(ch[0]*2, ch[0])\n",
        "        self.head = nn.Conv2d(ch[0], out_ch, 1)\n",
        "    def forward(self, x):\n",
        "        x1=self.d1(x); x2=self.d2(self.p1(x1)); x3=self.d3(self.p2(x2)); x4=self.d4(self.p3(x3))\n",
        "        y=self.u3(x4); y=self.dc3(torch.cat([y,x3],dim=1))\n",
        "        y=self.u2(y);  y=self.dc2(torch.cat([y,x2],dim=1))\n",
        "        y=self.u1(y);  y=self.dc1(torch.cat([y,x1],dim=1))\n",
        "        return torch.sigmoid(self.head(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== Loss & Eval =====\n",
        "class DepthLoss(nn.Module):\n",
        "    def __init__(self, w_l1=1.0, w_silog=0.1):\n",
        "        super().__init__()\n",
        "        self.w_l1=float(w_l1); self.w_silog=float(w_silog)\n",
        "    def silog(self, pred, gt, mask, eps=1e-6, lam=0.85):\n",
        "        p = pred[mask].clamp_min(eps); g = gt[mask].clamp_min(eps)\n",
        "        d = torch.log(p) - torch.log(g)\n",
        "        return torch.mean(d**2) - lam*(torch.mean(d)**2)\n",
        "    def forward(self, pred, gt, mask):\n",
        "        loss = 0.0\n",
        "        if self.w_l1>0: loss = loss + self.w_l1 * F.l1_loss(pred[mask], gt[mask])\n",
        "        if self.w_silog>0 and mask.any(): loss = loss + self.w_silog * self.silog(pred, gt, mask)\n",
        "        return loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, max_depth):\n",
        "    model.eval(); rmses=[]; absrels=[]\n",
        "    for batch in loader:\n",
        "        x,y,m = to_device(batch, device)\n",
        "        with torch.autocast(device.type if device.type!='cpu' else 'cpu',\n",
        "                            dtype=torch.float16 if device.type=='cuda' else torch.bfloat16,\n",
        "                            enabled=(device.type!='cpu')):\n",
        "            p = model(x)\n",
        "        p_m = p * max_depth; y_m = y * max_depth\n",
        "        m2 = m & torch.isfinite(y_m) & (y_m>0)\n",
        "        rmses.append(rmse(p_m, y_m, m2).item())\n",
        "        absrels.append(abs_rel(p_m, y_m, m2).item())\n",
        "    return float(np.nanmean(rmses)), float(np.nanmean(absrels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wilson\\AppData\\Local\\Temp\\ipykernel_47176\\4193635566.py:43: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples: 6639 val samples: 427\n",
            "batches per epoch -> train: 414 val: 27\n",
            "Device=cuda; img_size=384 batch=16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50:   1%|          | 4/414 [00:09<15:43,  2.30s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m running \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     54\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(x,y,m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m     56\u001b[0m     x,y,m \u001b[38;5;241m=\u001b[39m to_device((x,y,m), device)\n\u001b[0;32m     57\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Users\\wilson\\anaconda3\\envs\\depth_train\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[1;32md:\\Users\\wilson\\anaconda3\\envs\\depth_train\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
            "File \u001b[1;32md:\\Users\\wilson\\anaconda3\\envs\\depth_train\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32md:\\Users\\wilson\\anaconda3\\envs\\depth_train\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32md:\\Users\\wilson\\anaconda3\\envs\\depth_train\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[3], line 36\u001b[0m, in \u001b[0;36mImageDepthNPY.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m pil \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeom_train(pil)\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor_norm(pil)\n\u001b[1;32m---> 36\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep_p\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     37\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(d)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     38\u001b[0m d \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(d, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32md:\\Users\\wilson\\anaconda3\\envs\\depth_train\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x300 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ===== Train with tqdm batch bar =====\n",
        "from tqdm import tqdm\n",
        "import platform\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Windows + Notebook 最穩：num_workers=0, pin_memory=False\n",
        "num_workers = 0\n",
        "pin_mem = False\n",
        "if platform.system() == \"Linux\" and device.type == \"cuda\":\n",
        "    # 你在 Linux server 可試著開高一點\n",
        "    num_workers = 4\n",
        "    pin_mem = True\n",
        "\n",
        "train_set = ImageDepthNPY(config['train_images'], config['train_depths'], config['img_size'], config['max_depth'], aug=True)\n",
        "val_set   = ImageDepthNPY(config['val_images'],   config['val_depths'],   config['img_size'], config['max_depth'], aug=False)\n",
        "\n",
        "print(\"train samples:\", len(train_set), \"val samples:\", len(val_set))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set, batch_size=config['batch'], shuffle=True,\n",
        "    num_workers=num_workers, pin_memory=pin_mem, drop_last=True\n",
        ")\n",
        "val_loader   = DataLoader(\n",
        "    val_set, batch_size=config['batch'], shuffle=False,\n",
        "    num_workers=num_workers, pin_memory=pin_mem\n",
        ")\n",
        "print(\"batches per epoch -> train:\", len(train_loader), \"val:\", len(val_loader))\n",
        "\n",
        "model = UNetSmall(in_ch=1).to(device)\n",
        "loss_fn = DepthLoss(config['w_l1'], config['w_silog'])\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "\n",
        "def lr_lambda(e):\n",
        "    warm=max(3,int(0.1*config['epochs']))\n",
        "    if e < warm: return (e+1)/warm\n",
        "    prog=(e-warm)/max(1, config['epochs']-warm)\n",
        "    return 0.5*(1+math.cos(math.pi*prog))\n",
        "sch = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "outdir = Path(config['out_dir']); outdir.mkdir(parents=True, exist_ok=True)\n",
        "best_rmse=float('inf')\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "hist = { 'train_loss': [], 'val_rmse': [], 'val_absrel': [] }\n",
        "\n",
        "# 移除 clear_output 的干擾（先專心看進度）\n",
        "fig,axs = plt.subplots(1,3, figsize=(12,3))\n",
        "print(f\"Device={device}; img_size={config['img_size']} batch={config['batch']}\")\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\", leave=True)\n",
        "    for i,(x,y,m) in enumerate(pbar):\n",
        "        x,y,m = to_device((x,y,m), device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device.type if device.type!='cpu' else 'cpu',\n",
        "                            dtype=torch.float16 if device.type=='cuda' else torch.bfloat16,\n",
        "                            enabled=(device.type!='cpu')):\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y, m)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt); scaler.update()\n",
        "        running += loss.item() * x.size(0)\n",
        "\n",
        "        # 在進度條上顯示目前 loss\n",
        "        if (i+1) % 10 == 0:\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{sch.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    sch.step()\n",
        "    tr_loss = running / len(train_loader.dataset)\n",
        "\n",
        "    # 驗證\n",
        "    val_rmse, val_absrel = evaluate(model, val_loader, device, config['max_depth'])\n",
        "    hist['train_loss'].append(tr_loss); hist['val_rmse'].append(val_rmse); hist['val_absrel'].append(val_absrel)\n",
        "\n",
        "    # 存 ckpt\n",
        "    torch.save({'epoch': epoch, 'model': model.state_dict()}, outdir/'last.pt')\n",
        "    if val_rmse < best_rmse:\n",
        "        best_rmse = val_rmse\n",
        "        torch.save({'epoch': epoch, 'model': model.state_dict()}, outdir/'best.pt')\n",
        "\n",
        "    # 畫曲線（不清空輸出）\n",
        "    axs[0].cla(); axs[1].cla(); axs[2].cla()\n",
        "    axs[0].plot(hist['train_loss']); axs[0].set_title('train loss')\n",
        "    axs[1].plot(hist['val_rmse']);   axs[1].set_title('val RMSE')\n",
        "    axs[2].plot(hist['val_absrel']); axs[2].set_title('val AbsRel')\n",
        "    plt.tight_layout(); display(fig)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} done. train {tr_loss:.4f} | val RMSE {val_rmse:.3f} AbsRel {val_absrel:.3f} | best RMSE {best_rmse:.3f}\")\n",
        "\n",
        "fig.savefig(outdir/'curves.png', dpi=150)\n",
        "print(\"Training finished. Best RMSE =\", best_rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be87cd0a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device=cuda; images=86; saving to output_npy_color\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "infer: 100%|██████████| 86/86 [00:05<00:00, 14.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Wrote 86 .npy to output_npy_color and heatmaps to output_npy_color\\viz, stacks to output_npy_color\\stack\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === Inference (folder) -> .npy + heatmap PNG (INFERNO) ===\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========= 填這些參數 =========\n",
        "ckpt_path      = r\"runs/dep_unet_v2/best.pt\"     # 你的 best.pt\n",
        "in_images_dir  = r\"0703_image\"                   # 要推論的影像資料夾\n",
        "out_dir        = r\"output_npy_color\"                   # 輸出資料夾（自動建立）\n",
        "img_size       = 384                             # 與訓練一致\n",
        "max_depth      = 80.0                            # 與訓練一致\n",
        "device_str     = \"cuda\"                          # \"cuda\" 或 \"cpu\"\n",
        "save_stack     = True                            # 另存 原圖/熱力圖 上下拼接 PNG\n",
        "\n",
        "# ========= 若 notebook 尚未有 UNetSmall，這裡補上最小相容模型 =========\n",
        "try:\n",
        "    UNetSmall\n",
        "except NameError:\n",
        "    class DoubleConv(nn.Module):\n",
        "        def __init__(self, in_c, out_c):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, 3, padding=1), nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_c, out_c, 3, padding=1), nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "            )\n",
        "        def forward(self, x): return self.net(x)\n",
        "\n",
        "    class UNetSmall(nn.Module):\n",
        "        def __init__(self, in_ch=1, out_ch=1):\n",
        "            super().__init__()\n",
        "            ch=[64,128,256,512]\n",
        "            self.d1=DoubleConv(in_ch,ch[0]); self.p1=nn.MaxPool2d(2)\n",
        "            self.d2=DoubleConv(ch[0],ch[1]); self.p2=nn.MaxPool2d(2)\n",
        "            self.d3=DoubleConv(ch[1],ch[2]); self.p3=nn.MaxPool2d(2)\n",
        "            self.d4=DoubleConv(ch[2],ch[3])\n",
        "            self.u3=nn.ConvTranspose2d(ch[3],ch[2],2,2); self.dc3=DoubleConv(ch[2]*2,ch[2])\n",
        "            self.u2=nn.ConvTranspose2d(ch[2],ch[1],2,2); self.dc2=DoubleConv(ch[1]*2,ch[1])\n",
        "            self.u1=nn.ConvTranspose2d(ch[1],ch[0],2,2); self.dc1=DoubleConv(ch[0]*2,ch[0])\n",
        "            self.head=nn.Conv2d(ch[0],out_ch,1)\n",
        "        def forward(self,x):\n",
        "            x1=self.d1(x); x2=self.d2(self.p1(x1)); x3=self.d3(self.p2(x2)); x4=self.d4(self.p3(x3))\n",
        "            y=self.u3(x4); y=self.dc3(torch.cat([y,x3],1))\n",
        "            y=self.u2(y);  y=self.dc2(torch.cat([y,x2],1))\n",
        "            y=self.u1(y);  y=self.dc1(torch.cat([y,x1],1))\n",
        "            return torch.sigmoid(self.head(y))\n",
        "\n",
        "def load_ckpt(ckpt_path, device):\n",
        "    ck = torch.load(ckpt_path, map_location=device)\n",
        "    model = UNetSmall(in_ch=1).to(device)\n",
        "    sd = ck[\"model\"] if isinstance(ck, dict) and \"model\" in ck else ck\n",
        "    model.load_state_dict(sd); model.eval()\n",
        "    return model\n",
        "\n",
        "# ========= 推論主程式 =========\n",
        "device = torch.device(\"cuda\" if (device_str==\"cuda\" and torch.cuda.is_available()) else \"cpu\")\n",
        "model  = load_ckpt(ckpt_path, device)\n",
        "\n",
        "in_dir  = Path(in_images_dir)\n",
        "out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "viz_dir = out_dir / \"viz\"; viz_dir.mkdir(parents=True, exist_ok=True)\n",
        "stack_dir = out_dir / \"stack\"; \n",
        "if save_stack: stack_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "exts = (\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\",\".webp\")\n",
        "imgs = sorted([p for p in in_dir.iterdir() if p.is_file() and p.suffix.lower() in exts])\n",
        "if not imgs:\n",
        "    raise FileNotFoundError(f\"No images in {in_dir}\")\n",
        "\n",
        "tf = T.Compose([\n",
        "    T.Resize((img_size, img_size)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.5,), std=(0.5,)),   # 與訓練一致（灰階）\n",
        "])\n",
        "\n",
        "print(f\"Device={device}; images={len(imgs)}; saving to {out_dir}\")\n",
        "for p in tqdm(imgs, desc=\"infer\"):\n",
        "    pil = Image.open(p).convert(\"L\")\n",
        "    W0, H0 = pil.size\n",
        "    x = tf(pil).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.autocast(device.type if device.type!=\"cpu\" else \"cpu\",\n",
        "                        dtype=torch.float16 if device.type==\"cuda\" else torch.bfloat16,\n",
        "                        enabled=(device.type!=\"cpu\")):\n",
        "        y = model(x)                    # [1,1,h,w], in [0,1]\n",
        "\n",
        "    # 還原到 metric depth & 原圖大小\n",
        "    y = (y * max_depth).squeeze(0)      # [1,h,w]\n",
        "    y = F.interpolate(y.unsqueeze(0), size=(H0, W0), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "    depth = y.squeeze(0).detach().cpu().numpy().astype(np.float32)   # [H,W]\n",
        "\n",
        "    # 存 .npy\n",
        "    np.save(out_dir / f\"{p.stem}.npy\", depth)\n",
        "\n",
        "    # 依你之前方法：裁到 [0,max_depth] -> 0~255 -> INFERNO colormap\n",
        "    depth_vis = np.clip(depth, 0.0, max_depth)\n",
        "    depth_u8  = np.round(depth_vis / max_depth * 255.0).astype(np.uint8)\n",
        "    heatmap   = cv2.applyColorMap(depth_u8, cv2.COLORMAP_INFERNO)\n",
        "    cv2.imwrite(str(viz_dir / f\"{p.stem}_heatmap.png\"), heatmap)\n",
        "\n",
        "    # 可選：與原圖上下拼接（原圖為 BGR）\n",
        "    if save_stack:\n",
        "        img_bgr = cv2.cvtColor(np.array(pil.resize((W0, H0))), cv2.COLOR_GRAY2BGR)\n",
        "        stacked = np.vstack([img_bgr, heatmap])\n",
        "        cv2.imwrite(str(stack_dir / f\"{p.stem}_stack.png\"), stacked)\n",
        "\n",
        "print(f\"Done. Wrote {len(imgs)} .npy to {out_dir} and heatmaps to {viz_dir}\" + (f\", stacks to {stack_dir}\" if save_stack else \"\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "depth_train",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
